{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Built neural nets from the scratch to implement output encoding using a vector of categorical variable combinations. This method is clearly explained in www.elsevier.com/locate/neunet. This method is compared against 1-hot encoding and other simple ways of using categorical inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Sharath - Machine Learning - Re-engineering neural nets\"\"\"\n",
    "import theano as th\n",
    "from theano import tensor as T\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import auc, accuracy_score\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from itertools import permutations, combinations_with_replacement, product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression on Car dataset:\n",
    "\n",
    "Comparing different methods developed for categorical variable usage in neural nets.\n",
    "The following code tries to implement and compare separation method described in the below paper to use categorical input features effectively in feed forward neural nets with existing techniques. \n",
    "Original paper \"A feed-forward network for input that is both categorical and quantitative Roelof K. Brouwer\"\n",
    "Neural Networks 15 (2002) 881â€“890\n",
    "www.elsevier.com/locate/neunet\n",
    "\n",
    "I have used car dataset from https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data to demonstrate the method\n",
    "\n",
    "Features in car dataset:\n",
    "buying       v-high, high, med, low\n",
    "maint        v-high, high, med, low\n",
    "doors        2, 3, 4, 5-more\n",
    "persons      2, 4, more\n",
    "lug_boot     small, med, big\n",
    "safety       low, med, high\n",
    "\n",
    "Method1: Convert categories to integers\n",
    "\n",
    "Method2: 1-hot encoding or dummy variables\n",
    "\n",
    "Method3: Separation method - Encode the output using the combination of categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "car = pd.read_csv(\"car.csv\", index_col=False, header=None)\n",
    "car.columns = [\"buying\",\"maint\",\"doors\",\"persons\",\"lug_boot\",\"safety\",\"target\"]\n",
    "car.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the variable types from character to factor. But first we will change them to integers for easy handling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for col in car.columns:\n",
    "    car[col], indexer = pd.factorize(car[col])\n",
    "    car[col] = car[col]\n",
    "print car.dtypes"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Method1: Plain vanilla neralnet considering all categorical variable levels as integers. \n",
    "Objective is to predict the satisfaction level (0 being unsatisfied and 3 being highly satisfied) - A regression prob now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Packages for neural net\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1209, 6) (1209,)\n",
      "count     1728\n",
      "unique       4\n",
      "top          0\n",
      "freq      1210\n",
      "Name: target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#data prep\n",
    "X = car.ix[:,:6]\n",
    "y = car.ix[:,6]\n",
    "Xtr, Xts, ytr, yts = train_test_split(X,y, test_size = 0.3)\n",
    "print Xtr.shape, ytr.shape\n",
    "print y.astype(object).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #Binarize the target variable for later use\n",
    "# lb = LabelBinarizer()\n",
    "# lb.fit(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1209, 6) (1209,)\n"
     ]
    }
   ],
   "source": [
    "# nb_classes = 4\n",
    "# ytr = np_utils.to_categorical(ytr,nb_classes)\n",
    "# yts = np_utils.to_categorical(yts, nb_classes)\n",
    "ytr = np.array(ytr, dtype=np.int32)\n",
    "yts = np.array(yts, dtype=np.int32)\n",
    "\n",
    "Xtr = np.array(Xtr, dtype=np.float32)\n",
    "Xts = np.array(Xts, dtype=np.float32)\n",
    "print Xtr.shape, ytr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def my_rmse(y, y_hat):\n",
    "    return T.sqrt(T.mean(T.square(y-y_hat), axis = -1))\n",
    "# Network Architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim = 6))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(20))\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "model.add(Dense(1))\n",
    "adam = Adam(lr=1e-2)\n",
    "model.compile(optimizer = adam, loss = my_rmse)\n",
    "#model.compile(loss = \"categorical_crossentropy\", optimizer = rms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#training the network\n",
    "batch_size = 10\n",
    "nb_epoch = 100\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=5, verbose=0, mode='auto')\n",
    "fit = model.fit(Xtr, ytr,\n",
    "         nb_epoch = nb_epoch,\n",
    "         batch_size = batch_size,\n",
    "         show_accuracy = True,\n",
    "         verbose = 0, callbacks = [earlyStopping],\n",
    "         validation_data = (Xts, yts)) #acheived accuracy on test = 0.94"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "519/519 [==============================] - 0s     \n"
     ]
    }
   ],
   "source": [
    "#prediction and validation\n",
    "yts_pred = model.predict(Xts, batch_size=batch_size, verbose=1)\n",
    "#print (\"test accuracy is: %.3f\" %(np.sqrt(np.mean(np.square(yts-t), axis=-1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fit.history[\"val_loss\"][-1]\n",
    "mse1 = np.mean([np.square(a1-b1) for a1,b1 in zip(yts_pred.tolist(),yts)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse in method1: 0.119865217343\n"
     ]
    }
   ],
   "source": [
    "print \"mse in method1:\", mse1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 2: Use binarized form for categorical variables as inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1728, 16)\n"
     ]
    }
   ],
   "source": [
    "car_num = car.ix[:,[\"doors\",\"persons\"]]\n",
    "car_cat = car[[\"buying\",\"maint\",\"lug_boot\",\"safety\"]]\n",
    "\n",
    "#Create dummy variable from categorical features\n",
    "car_cat = pd.get_dummies(car_cat.astype(str))\n",
    "#add back to main df\n",
    "car_with_dummies = pd.concat([car_num, car_cat], axis=1)\n",
    "print car_with_dummies.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1209, 16) (1209,)\n"
     ]
    }
   ],
   "source": [
    "Xtr, Xts, ytr, yts = train_test_split(car_with_dummies,y, test_size = 0.3)\n",
    "print Xtr.shape, ytr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1209, 16) (1209,)\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for a Network\n",
    "ytr = np.array(ytr, dtype=np.int32)\n",
    "yts = np.array(yts, dtype=np.int32)\n",
    "\n",
    "Xtr = np.array(Xtr, dtype=np.float32)\n",
    "Xts = np.array(Xts, dtype=np.float32)\n",
    "print Xtr.shape, ytr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def my_rmse(y, y_hat):\n",
    "    return T.sqrt(T.mean(T.square(y-y_hat), axis = -1))\n",
    "# Network Architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim = 16))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(20))\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "model.add(Dense(1))\n",
    "adam = Adam(lr=1e-2)\n",
    "model.compile(optimizer = adam, loss = my_rmse)\n",
    "#model.compile(loss = \"categorical_crossentropy\", optimizer = rms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#training the network\n",
    "batch_size = 10\n",
    "nb_epoch = 100\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=5, verbose=0, mode='auto')\n",
    "fit = model.fit(Xtr, ytr,\n",
    "         nb_epoch = nb_epoch,\n",
    "         batch_size = batch_size,\n",
    "         show_accuracy = True,\n",
    "         verbose = 0, callbacks = [earlyStopping],\n",
    "         validation_data = (Xts, yts)) #acheived accuracy on test = 0.965"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "519/519 [==============================] - 0s     \n"
     ]
    }
   ],
   "source": [
    "#prediction and validation\n",
    "yts_pred2 = model.predict(Xts, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fit.history[\"val_loss\"][-1]\n",
    "mse2 = np.mean([np.square(a1-b1) for a1,b1 in zip(yts_pred2.tolist(),yts)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse in method 2 is: 0.0848446252746\n"
     ]
    }
   ],
   "source": [
    "print \"mse in method 2 is:\", mse2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 3: Separation Method As described in the paper.\n",
    "Let us look at levels in categorical variables. In our separation method, we have to consider 4X4X3X3 = 144 element vector to encode the output layer.\n",
    "s vector = 144 length\n",
    "\n",
    "number of nodes in output layer = 144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create numerical and categorical feature sets\n",
    "car_num = car.ix[:,[\"doors\",\"persons\"]]\n",
    "car_cat = car[[\"buying\",\"maint\",\"lug_boot\",\"safety\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating svector: \n",
    "1. There can be 144 unique combinations from categorical variables\n",
    "2. We need to encode each row in categorical data with one of the 144 unique combinations\n",
    "3. Create a 144 length binarized vector from each encoded row\n",
    "4. Need to come up with much simpler method later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique combinations of categorical variables are 144\n",
      "1728 rows encoded\n"
     ]
    }
   ],
   "source": [
    "#Creating s-vector as described in the paper\n",
    "levels = []\n",
    "for col in car_cat.columns:\n",
    "    levels.append(list(set(car_cat[col])))\n",
    "levels\n",
    "svector = list(product(levels[0],levels[1], levels[2], levels[3]))\n",
    "print \"Unique combinations of categorical variables are %d\" %len(svector)\n",
    "\n",
    "#converting it into a dictionary\n",
    "svec = {}\n",
    "for idx, val in enumerate(svector):\n",
    "    svec[val] = idx\n",
    "\n",
    "#encoding each row in categorical data with one of the 144 unique comb\n",
    "combined = []\n",
    "for idx, row in car_cat.iterrows():\n",
    "    combined.append(svec[tuple([row[0], row[1], row[2], row[3]])])\n",
    "print \"%d rows encoded\" %len(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1728, 144)\n",
      "for every row, categorical value encoder of 144 length created\n"
     ]
    }
   ],
   "source": [
    "# We need to binarize the svector\n",
    "lb_svec = LabelBinarizer()\n",
    "lb_svec.fit(svec.values())\n",
    "svec = lb_svec.fit_transform(combined)\n",
    "\n",
    "print svec.shape\n",
    "print \"for every row, categorical value encoder of %d length created\" %(svec.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing the data for method 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1728, 2) (1728,)\n"
     ]
    }
   ],
   "source": [
    "X = car_num\n",
    "X = np.array(X, dtype=np.float32)\n",
    "y = car.ix[:,\"target\"]\n",
    "y = np.array(y)\n",
    "print X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train and test splits:  (1209, 2) (519, 2) (1209,) (519,)\n",
      "svector for train and test: (1209, 144) (519, 144)\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for method 3\n",
    "sample = np.random.permutation(X.shape[0])\n",
    "t = 0.7*len(sample)\n",
    "Xtr, Xts = X[sample[:t],], X[sample[t:],]\n",
    "ytr, yts = y[sample[:t]], y[sample[t:]]\n",
    "print \"train and test splits: \", Xtr.shape, Xts.shape, ytr.shape, yts.shape\n",
    "#Categorical encoder\n",
    "s_tr, s_ts = svec[sample[:t,]], svec[sample[t:,]]\n",
    "print \"svector for train and test:\",s_tr.shape, s_ts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMP: Please create the Class object and Activation Functions below before training the net\n",
    "\n",
    "Network Architecture: \n",
    "Input layer with 2 neurons\n",
    "2 hidden layers with 10 and 20 neurons.\n",
    "output layer with 144 neurons\n",
    "\n",
    "Trained 100 epochs and 0.1 lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param=((2,0,0),(10, logistic, logistic_prime),(20, logistic, logistic_prime),(144,identity, identity_prime))\n",
    "#Set learning rate.\n",
    "rates=[0.05]\n",
    "net = NeuralNetwork(Xtr,ytr,s_tr,param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net.train(200,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = net.predict(Xts, s_ts)\n",
    "pred = np.sum(pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mse3 = np.mean([np.square(a1-b1) for a1,b1 in zip(pred.tolist(),yts)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse in method 3 is: 0.0768177149449\n"
     ]
    }
   ],
   "source": [
    "print \"mse in method 3 is:\", mse3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Most of this I learned from https://triangleinequality.wordpress.com/2014/03/31/neural-networks-part-2/.\n",
    "#But built the code as per Separation methodology mentioned in www.elsevier.com/locate/neunet\n",
    "class NeuralNetwork(object):\n",
    "    def __init__(self, X, y, svec, parameters):\n",
    "        #Input data\n",
    "        self.X=X\n",
    "        #Output data\n",
    "        self.y=y\n",
    "        #Categorical vector for output encoding\n",
    "        self.svec = svec\n",
    "        \n",
    "        #Expect parameters to be a tuple of the form:\n",
    "        self.n_layers = len(parameters)\n",
    "        #Counts number of neurons without bias neurons in each layer.\n",
    "        self.sizes = [layer[0] for layer in parameters]\n",
    "        #Activation functions for each layer.\n",
    "        self.fs =[layer[1] for layer in parameters]\n",
    "        #Derivatives of activation functions for each layer.\n",
    "        self.fprimes = [layer[2] for layer in parameters]\n",
    "        self.build_network()\n",
    " \n",
    "    def build_network(self):\n",
    "        #List of weight matrices taking the output of one layer to the input of the next.\n",
    "        self.weights=[]\n",
    "        #Bias vector for each layer.\n",
    "        self.biases=[]\n",
    "        #Input vector for each layer.\n",
    "        self.inputs=[]\n",
    "        #Output vector for each layer.\n",
    "        self.outputs=[]\n",
    "        #Vector of errors at each layer.\n",
    "        self.errors=[]\n",
    "        #We initialise the weights randomly, and fill the other vectors with 1s.\n",
    "        for layer in range(self.n_layers-1):\n",
    "            n = self.sizes[layer]\n",
    "            m = self.sizes[layer+1]\n",
    "            self.weights.append(np.random.normal(0,1, (m,n)))\n",
    "            self.biases.append(np.random.normal(0,1,(m,1)))\n",
    "            self.inputs.append(np.zeros((n,1)))\n",
    "            self.outputs.append(np.zeros((n,1)))\n",
    "            self.errors.append(np.zeros((n,1)))\n",
    "        #There are only n-1 weight matrices, so we do the last case separately.\n",
    "        n = self.sizes[-1]\n",
    "        self.inputs.append(np.zeros((n,1)))\n",
    "        self.outputs.append(np.zeros((n,1)))\n",
    "        self.errors.append(np.zeros((n,1)))\n",
    " \n",
    "    def feedforward(self, x):\n",
    "        #Propagates the input from the input layer to the output layer.\n",
    "        k=len(x)\n",
    "        x.shape=(k,1)\n",
    "        self.inputs[0]=x\n",
    "        self.outputs[0]=x\n",
    "        for i in range(1,self.n_layers):\n",
    "            self.inputs[i]=self.weights[i-1].dot(self.outputs[i-1])+self.biases[i-1]\n",
    "            self.outputs[i]=self.fs[i](self.inputs[i])\n",
    "        return self.outputs[-1]\n",
    " \n",
    "    def update_weights(self,x,y,s):\n",
    "        #Update the weight matrices for each layer based on a single input x and target y.\n",
    "        output = self.feedforward(x)\n",
    "        \n",
    "        #Encode the output with categorical vector \n",
    "        output = output.T*s\n",
    "        self.errors[-1]=self.fprimes[-1](self.outputs[-1])*(output- s*y)\n",
    "        self.errors[-1]=self.errors[-1].T\n",
    "        #print output.shape\n",
    " \n",
    "        n=self.n_layers-2\n",
    "        for i in xrange(n,0,-1):\n",
    "            self.errors[i] = self.fprimes[i](self.inputs[i])*self.weights[i].T.dot(self.errors[i+1])\n",
    "            #print self.outputs[i], self.errors[i+1].shape\n",
    "            self.weights[i] = self.weights[i]-self.learning_rate*np.outer(self.errors[i+1],self.outputs[i])\n",
    "            self.biases[i] = self.biases[i] - self.learning_rate*self.errors[i+1]\n",
    "        self.weights[0] = self.weights[0]-self.learning_rate*np.outer(self.errors[1],self.outputs[0])\n",
    "        self.biases[0] = self.biases[0] - self.learning_rate*self.errors[1] \n",
    "    \n",
    "    def train(self,n_iter, learning_rate=1):\n",
    "        #Updates the weights after comparing each input in X with y\n",
    "        #repeats this process n_iter times.\n",
    "        self.learning_rate=learning_rate\n",
    "        n=self.X.shape[0]\n",
    "        for repeat in range(n_iter):\n",
    "            #We shuffle the order in which we go through the inputs on each iter.\n",
    "            index=list(range(n))\n",
    "            np.random.shuffle(index)\n",
    "            for row in index:\n",
    "                x=self.X[row]\n",
    "                y=self.y[row]\n",
    "                s=self.svec[row]\n",
    "                self.update_weights(x,y,s)\n",
    " \n",
    "    def predict_x(self, x):\n",
    "        return self.feedforward(x)\n",
    " \n",
    "    def predict(self,X,svec):\n",
    "        n = len(X)\n",
    "        m = self.sizes[-1]\n",
    "        ret = np.ones((n,m))\n",
    "        for i in range(len(X)):\n",
    "            output = self.feedforward(X[i])\n",
    "            #print output.shape, svec[i,].shape\n",
    "            ret[i,:] = output.T*svec[i,]\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic(x):\n",
    "    return 1.0/(1+np.exp(-x))\n",
    " \n",
    "def logistic_prime(x):\n",
    "    ex=np.exp(-x)\n",
    "    return ex/(1+ex)**2\n",
    " \n",
    "def identity(x):\n",
    "    return x\n",
    " \n",
    "def identity_prime(x):\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
